{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "title_cell",
            "metadata": {},
            "source": [
                "# Eksperimen Supply Chain Demand Forecasting\n",
                "**Nama**: I Made Wisnu Adi Sanjaya  \n",
                "**Proyek**: MLOps - Retail Demand Prediction\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "intro_dataset",
            "metadata": {},
            "source": [
                "## 1. Perkenalan Dataset\n",
                "\n",
                "### Informasi Umum\n",
                "- **Nama Dataset**: Supply Chain Demand Forecasting\n",
                "- **Sumber**: [Kaggle - Demand Forecasting](https://www.kaggle.com/datasets/aswathrao/demand-forecasting)\n",
                "- **Tujuan**: Membangun model forecasting untuk memprediksi jumlah unit yang terjual (`units_sold`) guna mengoptimalkan manajemen stok dan supply chain\n",
                "\n",
                "### Deskripsi\n",
                "Dataset ini berisi riwayat transaksi penjualan selama **2-3 tahun** untuk berbagai produk (SKU) di **10 toko** berbeda. Data mencakup informasi temporal, lokasi, dan pricing yang dapat digunakan untuk memprediksi permintaan produk.\n",
                "\n",
                "### Atribut Dataset\n",
                "| Kolom | Tipe | Deskripsi |\n",
                "|-------|------|----------|\n",
                "| `week` | Date | Tanggal/minggu penjualan |\n",
                "| `store_id` | Categorical | ID unik toko |\n",
                "| `sku_id` | Categorical | ID produk (Stock Keeping Unit) |\n",
                "| `base_price` | Numeric | Harga dasar produk |\n",
                "| `total_price` | Numeric | Harga akhir setelah promosi/diskon |\n",
                "| `units_sold` | Numeric | **Target**: Jumlah unit terjual (yang ingin diprediksi) |\n",
                "\n",
                "### Use Case\n",
                "Model forecasting ini akan membantu retailer dalam:\n",
                "- Optimalisasi inventory management\n",
                "- Mencegah stockout atau overstock\n",
                "- Perencanaan supply chain yang lebih akurat\n",
                "- Strategi pricing dan promosi yang data-driven\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "import_section",
            "metadata": {},
            "source": [
                "## 2. Import Library\n",
                "\n",
                "Mengimpor library yang dibutuhkan untuk:\n",
                "- **Data Processing**: pandas, numpy\n",
                "- **Visualization**: matplotlib, seaborn\n",
                "- **Encoding**: category_encoders\n",
                "- **Persistence**: joblib\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "import_cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import joblib\n",
                "import os\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from category_encoders import MEstimateEncoder\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set style untuk visualisasi\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (10, 6)\n",
                "\n",
                "# Import config (fallback jika tidak ada)\n",
                "try:\n",
                "    from config import SKU_SPECIFIC_LAGS, SKU_SPECIFIC_MAS\n",
                "    print(\"✓ Config loaded successfully\")\n",
                "except ImportError:\n",
                "    print(\"⚠ Config.py not found, using default values\")\n",
                "    SKU_SPECIFIC_LAGS = {216418: [1, 2, 3]}\n",
                "    SKU_SPECIFIC_MAS = {216418: [2, 4]}\n",
                "\n",
                "print(f\"Target SKU: {list(SKU_SPECIFIC_LAGS.keys())}\")\n",
                "print(f\"Lag features: {SKU_SPECIFIC_LAGS}\")\n",
                "print(f\"Moving average windows: {SKU_SPECIFIC_MAS}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "load_section",
            "metadata": {},
            "source": [
                "## 3. Memuat Dataset\n",
                "\n",
                "Memuat data training dari file CSV ke dalam pandas DataFrame."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load dataset dengan error handling\n",
                "try:\n",
                "    train = pd.read_csv('../data/train.csv')\n",
                "    print(\"✓ Dataset loaded successfully from ../data/train.csv\")\n",
                "except FileNotFoundError:\n",
                "    if os.path.exists('data/train.csv'):\n",
                "        train = pd.read_csv('data/train.csv')\n",
                "        print(\"✓ Dataset loaded successfully from data/train.csv\")\n",
                "    else:\n",
                "        raise FileNotFoundError(\"train.csv not found in expected locations\")\n",
                "\n",
                "print(f\"\\nDataset Shape: {train.shape}\")\n",
                "print(f\"Total Records: {len(train):,}\")\n",
                "print(f\"Total Features: {train.shape[1]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "preview_section",
            "metadata": {},
            "source": [
                "### Preview Data\n",
                "Melihat sampel data untuk memahami struktur dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "preview_cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Tampilkan 10 baris pertama\n",
                "train.head(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eda_section",
            "metadata": {},
            "source": [
                "## 4. Exploratory Data Analysis (EDA)\n",
                "\n",
                "Analisis mendalam terhadap dataset untuk memahami:\n",
                "- Struktur dan tipe data\n",
                "- Missing values\n",
                "- Distribusi statistik\n",
                "- Outliers\n",
                "- Pola temporal"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eda_basic_section",
            "metadata": {},
            "source": [
                "### 4.1 Informasi Dasar Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eda_info",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dataset info\n",
                "print(\"=\" * 60)\n",
                "print(\"DATASET INFORMATION\")\n",
                "print(\"=\" * 60)\n",
                "print(train.info())\n",
                "\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"COLUMN DATA TYPES\")\n",
                "print(\"=\" * 60)\n",
                "print(train.dtypes)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eda_stats_section",
            "metadata": {},
            "source": [
                "### 4.2 Statistik Deskriptif"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eda_stats",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Statistik deskriptif\n",
                "print(\"=\" * 60)\n",
                "print(\"DESCRIPTIVE STATISTICS - NUMERIC FEATURES\")\n",
                "print(\"=\" * 60)\n",
                "display(train.describe().T)\n",
                "\n",
                "# Statistik untuk categorical features\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"CATEGORICAL FEATURES SUMMARY\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"Unique Stores: {train['store_id'].nunique()}\")\n",
                "print(f\"Unique SKUs: {train['sku_id'].nunique()}\")\n",
                "print(f\"Date Range: {train['week'].min()} to {train['week'].max()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eda_missing_section",
            "metadata": {},
            "source": [
                "### 4.3 Data Quality Check"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eda_quality",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Missing values analysis\n",
                "print(\"=\" * 60)\n",
                "print(\"MISSING VALUES ANALYSIS\")\n",
                "print(\"=\" * 60)\n",
                "missing = train.isnull().sum()\n",
                "missing_pct = (missing / len(train) * 100).round(2)\n",
                "missing_df = pd.DataFrame({\n",
                "    'Missing Count': missing,\n",
                "    'Percentage (%)': missing_pct\n",
                "})\n",
                "display(missing_df[missing_df['Missing Count'] > 0])\n",
                "\n",
                "# Duplicates check\n",
                "duplicates = train.duplicated().sum()\n",
                "print(f\"\\nDuplicate Rows: {duplicates} ({duplicates/len(train)*100:.2f}%)\")\n",
                "\n",
                "# Zero values in target\n",
                "zero_units = (train['units_sold'] == 0).sum()\n",
                "print(f\"Zero Units Sold: {zero_units} ({zero_units/len(train)*100:.2f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eda_viz_section",
            "metadata": {},
            "source": [
                "### 4.4 Visualisasi Distribusi Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eda_distribution",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Distribution of target variable\n",
                "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
                "\n",
                "# Histogram\n",
                "axes[0].hist(train['units_sold'], bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
                "axes[0].set_xlabel('Units Sold', fontsize=12)\n",
                "axes[0].set_ylabel('Frequency', fontsize=12)\n",
                "axes[0].set_title('Distribution of Units Sold', fontsize=14, fontweight='bold')\n",
                "axes[0].axvline(train['units_sold'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {train[\"units_sold\"].mean():.2f}')\n",
                "axes[0].legend()\n",
                "\n",
                "# Boxplot\n",
                "axes[1].boxplot(train['units_sold'], vert=True, patch_artist=True,\n",
                "                boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
                "                medianprops=dict(color='red', linewidth=2))\n",
                "axes[1].set_ylabel('Units Sold', fontsize=12)\n",
                "axes[1].set_title('Boxplot of Units Sold (Outlier Detection)', fontsize=14, fontweight='bold')\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"Mean Units Sold: {train['units_sold'].mean():.2f}\")\n",
                "print(f\"Median Units Sold: {train['units_sold'].median():.2f}\")\n",
                "print(f\"Std Dev: {train['units_sold'].std():.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eda_price_dist",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Price distributions\n",
                "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
                "\n",
                "# Base price\n",
                "axes[0].hist(train['base_price'].dropna(), bins=30, edgecolor='black', alpha=0.7, color='lightgreen')\n",
                "axes[0].set_xlabel('Base Price', fontsize=12)\n",
                "axes[0].set_ylabel('Frequency', fontsize=12)\n",
                "axes[0].set_title('Distribution of Base Price', fontsize=14, fontweight='bold')\n",
                "\n",
                "# Total price\n",
                "axes[1].hist(train['total_price'].dropna(), bins=30, edgecolor='black', alpha=0.7, color='lightcoral')\n",
                "axes[1].set_xlabel('Total Price', fontsize=12)\n",
                "axes[1].set_ylabel('Frequency', fontsize=12)\n",
                "axes[1].set_title('Distribution of Total Price', fontsize=14, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eda_categorical_section",
            "metadata": {},
            "source": [
                "### 4.5 Analisis Kategorical Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eda_categorical",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Distribution by store and SKU\n",
                "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
                "\n",
                "# Sales by store\n",
                "store_sales = train.groupby('store_id')['units_sold'].sum().sort_values(ascending=False)\n",
                "axes[0].bar(range(len(store_sales)), store_sales.values, color='steelblue', alpha=0.7)\n",
                "axes[0].set_xlabel('Store ID (sorted by total sales)', fontsize=12)\n",
                "axes[0].set_ylabel('Total Units Sold', fontsize=12)\n",
                "axes[0].set_title('Total Sales by Store', fontsize=14, fontweight='bold')\n",
                "\n",
                "# Top 15 SKUs by sales\n",
                "sku_sales = train.groupby('sku_id')['units_sold'].sum().sort_values(ascending=False).head(15)\n",
                "axes[1].barh(range(len(sku_sales)), sku_sales.values, color='coral', alpha=0.7)\n",
                "axes[1].set_xlabel('Total Units Sold', fontsize=12)\n",
                "axes[1].set_ylabel('SKU ID (Top 15)', fontsize=12)\n",
                "axes[1].set_title('Top 15 SKUs by Total Sales', fontsize=14, fontweight='bold')\n",
                "axes[1].invert_yaxis()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nTop 3 Stores by Sales:\")\n",
                "print(store_sales.head(3))\n",
                "print(f\"\\nTop 3 SKUs by Sales:\")\n",
                "print(sku_sales.head(3))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "preprocessing_section",
            "metadata": {},
            "source": [
                "## 5. Data Preprocessing\n",
                "\n",
                "Tahapan pembersihan dan transformasi data:\n",
                "1. **Konversi Tipe Data**: Date parsing dan type conversion\n",
                "2. **Handling Missing Values**: Imputasi nilai yang hilang\n",
                "3. **Handling Duplicates**: Menghapus data duplikat\n",
                "4. **Encoding**: Target encoding untuk categorical features\n",
                "5. **Feature Engineering**: Membuat lag features dan moving averages untuk time series\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "preprocess_date_section",
            "metadata": {},
            "source": [
                "### 5.1 Konversi Date & Fix Leap Year\n",
                "Mengatasi masalah leap year yang menyebabkan error parsing tanggal."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "preprocess_date",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Step 1: Date Conversion & Leap Year Fix\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Convert to datetime\n",
                "train['week'] = pd.to_datetime(train['week'], format='%d/%m/%y')\n",
                "\n",
                "# Fix leap year issue (29 Feb 2012 -> 28 Feb 2012)\n",
                "train.loc[train['week'] >= '2012-03-06', 'week'] -= pd.Timedelta(days=1)\n",
                "\n",
                "print(f\"✓ Date converted successfully\")\n",
                "print(f\"Date Range: {train['week'].min()} to {train['week'].max()}\")\n",
                "print(f\"Total Days Covered: {(train['week'].max() - train['week'].min()).days}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "preprocess_missing_section",
            "metadata": {},
            "source": [
                "### 5.2 Handling Missing Values\n",
                "Mengisi missing values pada `total_price` dengan `base_price`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "preprocess_missing",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Step 2: Handling Missing Values\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "missing_before = train['total_price'].isnull().sum()\n",
                "train['total_price'] = train['total_price'].fillna(train['base_price'])\n",
                "missing_after = train['total_price'].isnull().sum()\n",
                "\n",
                "print(f\"✓ Missing values in total_price filled\")\n",
                "print(f\"Before: {missing_before} missing values\")\n",
                "print(f\"After: {missing_after} missing values\")\n",
                "print(f\"Filled: {missing_before - missing_after} values\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "preprocess_dup_section",
            "metadata": {},
            "source": [
                "### 5.3 Handling Duplicates"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "preprocess_dup",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Step 3: Handling Duplicates\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "initial_rows = len(train)\n",
                "train = train.drop_duplicates()\n",
                "final_rows = len(train)\n",
                "\n",
                "print(f\"✓ Duplicates removed\")\n",
                "print(f\"Initial rows: {initial_rows:,}\")\n",
                "print(f\"Final rows: {final_rows:,}\")\n",
                "print(f\"Removed: {initial_rows - final_rows:,} duplicate rows\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "preprocess_encoding_section",
            "metadata": {},
            "source": [
                "### 5.4 Target Encoding\n",
                "Menggunakan M-Estimate Encoder untuk mengubah `store_id` menjadi numerical value berdasarkan target variable."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "preprocess_encoding",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Step 4: Target Encoding\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Initialize encoder\n",
                "encoder = MEstimateEncoder(cols=['store_id'])\n",
                "encoder.fit(train[['store_id']], train['units_sold'])\n",
                "\n",
                "# Save encoder for inference\n",
                "os.makedirs('model_artifacts', exist_ok=True)\n",
                "joblib.dump(encoder, 'model_artifacts/store_encoder.pkl')\n",
                "\n",
                "# Transform\n",
                "train['store_encoded'] = encoder.transform(train[['store_id']])\n",
                "\n",
                "print(f\"✓ Target encoding completed\")\n",
                "print(f\"Encoder saved to: model_artifacts/store_encoder.pkl\")\n",
                "print(f\"\\nSample encoded values:\")\n",
                "display(train[['store_id', 'store_encoded']].drop_duplicates().head(10))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "preprocess_fe_section",
            "metadata": {},
            "source": [
                "### 5.5 Feature Engineering: Time Series Features\n",
                "\n",
                "Membuat features untuk time series forecasting:\n",
                "- **Lag Features**: Nilai sales pada t-1, t-2, t-3\n",
                "- **Moving Averages**: Rolling average untuk menangkap trend\n",
                "\n",
                "**Note**: Untuk efisiensi, kita filter hanya SKU tertentu sesuai config."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "preprocess_filter",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Step 5: Feature Engineering - Time Series Features\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Filter target SKUs\n",
                "target_skus = list(SKU_SPECIFIC_LAGS.keys())\n",
                "print(f\"Target SKUs for model: {target_skus}\")\n",
                "\n",
                "rows_before_filter = len(train)\n",
                "train = train[train['sku_id'].isin(target_skus)].copy()\n",
                "rows_after_filter = len(train)\n",
                "\n",
                "print(f\"\\nFiltering completed:\")\n",
                "print(f\"Before: {rows_before_filter:,} rows\")\n",
                "print(f\"After: {rows_after_filter:,} rows\")\n",
                "print(f\"Filtered out: {rows_before_filter - rows_after_filter:,} rows\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "preprocess_features",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Creating lag features and moving averages...\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "df_list = []\n",
                "\n",
                "for sku_id in target_skus:\n",
                "    print(f\"\\nProcessing SKU: {sku_id}\")\n",
                "    df_sku = train[train['sku_id'] == sku_id].copy().sort_values('week')\n",
                "    \n",
                "    # Create lag features\n",
                "    lags = SKU_SPECIFIC_LAGS.get(sku_id, [1])\n",
                "    print(f\"  Creating lag features: {lags}\")\n",
                "    for lag in lags:\n",
                "        df_sku[f'lag_{lag}'] = df_sku['units_sold'].shift(lag)\n",
                "    \n",
                "    # Create moving average features\n",
                "    mas = SKU_SPECIFIC_MAS.get(sku_id, [1])\n",
                "    print(f\"  Creating moving averages: {mas}\")\n",
                "    for window in mas:\n",
                "        df_sku[f'ma_{window}'] = df_sku['units_sold'].rolling(window=window).mean().shift(1)\n",
                "    \n",
                "    print(f\"  Rows for SKU {sku_id}: {len(df_sku)}\")\n",
                "    df_list.append(df_sku)\n",
                "\n",
                "# Concatenate all SKUs\n",
                "df_final = pd.concat(df_list, ignore_index=True)\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"Total rows before dropna: {len(df_final):,}\")\n",
                "\n",
                "# Remove rows with NaN (from lag/MA operations)\n",
                "df_final = df_final.dropna()\n",
                "print(f\"Total rows after dropna: {len(df_final):,}\")\n",
                "print(f\"Dropped: {len(pd.concat(df_list)) - len(df_final):,} rows with NaN\")\n",
                "\n",
                "print(f\"\\n✓ Feature engineering completed\")\n",
                "print(f\"Final dataset shape: {df_final.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "preprocess_result_section",
            "metadata": {},
            "source": [
                "### 5.6 Hasil Preprocessing\n",
                "Melihat preview data yang sudah diproses dan siap untuk modeling."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "preprocess_preview",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Final Processed Dataset Preview\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"Shape: {df_final.shape}\")\n",
                "print(f\"\\nFeatures: {df_final.columns.tolist()}\")\n",
                "print(f\"\\nSample Data:\")\n",
                "display(df_final.head(10))\n",
                "\n",
                "print(f\"\\nFeature Statistics:\")\n",
                "display(df_final.describe().T)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "save_section",
            "metadata": {},
            "source": [
                "## 6. Simpan Hasil\n",
                "\n",
                "Menyimpan data yang sudah diproses ke file CSV untuk digunakan pada tahap modeling."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "save_cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create output directory\n",
                "os.makedirs('data/processed', exist_ok=True)\n",
                "output_path = 'data/processed/train_processed.csv'\n",
                "\n",
                "# Save to CSV\n",
                "df_final.to_csv(output_path, index=False)\n",
                "\n",
                "print(\"=\" * 60)\n",
                "print(\"DATA PREPROCESSING COMPLETED\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"✓ Processed data saved to: {output_path}\")\n",
                "print(f\"\\nFinal Statistics:\")\n",
                "print(f\"  - Total Records: {len(df_final):,}\")\n",
                "print(f\"  - Total Features: {df_final.shape[1]}\")\n",
                "print(f\"  - File Size: {os.path.getsize(output_path) / 1024:.2f} KB\")\n",
                "print(f\"\\nData is ready for model training!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "summary_section",
            "metadata": {},
            "source": [
                "## 7. Summary\n",
                "\n",
                "### Preprocessing Pipeline Summary\n",
                "\n",
                "| Step | Action | Result |\n",
                "|------|--------|--------|\n",
                "| 1 | Date Conversion | Fixed leap year issue |\n",
                "| 2 | Missing Values | Filled total_price with base_price |\n",
                "| 3 | Duplicates | Removed duplicate rows |\n",
                "| 4 | Encoding | Applied M-Estimate encoding to store_id |\n",
                "| 5 | Feature Engineering | Created lag features & moving averages |\n",
                "| 6 | Filtering | Selected target SKUs for modeling |\n",
                "\n",
                "### Features Created\n",
                "- **Original Features**: week, store_id, sku_id, base_price, total_price, units_sold\n",
                "- **Engineered Features**: \n",
                "  - `store_encoded`: Target-encoded store values\n",
                "  - `lag_1`, `lag_2`, `lag_3`: Previous time step values\n",
                "  - `ma_2`, `ma_4`: Moving averages\n",
                "\n",
                "### Next Steps\n",
                "1. ✅ Data preprocessing completed\n",
                "2. → Model training dengan `modelling.py`\n",
                "3. → Hyperparameter tuning dengan `modelling_tuning.py`\n",
                "4. → Model deployment & monitoring\n",
                "\n",
                "---\n",
                "**End of Notebook**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}